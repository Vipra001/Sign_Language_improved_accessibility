{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434ac7f2-b4c4-41c7-ac72-4b182ade1e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow opencv-python scikit-learn matplotlib mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81736509-bbac-484d-8259-18009c176a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    " \n",
    "import cv2 \n",
    "import os \n",
    " \n",
    "# Create a folder for each gesture you want to recognize \n",
    "gestures = ['A', 'B', 'C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T,'U','V','W','X','Y','Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb96ea-ab4e-4c4c-9911-2bcd80ce4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they don't exist \n",
    "\n",
    "for gesture in gestures: \n",
    "    if not os.path.exists(f'dataset/{gesture}'): \n",
    "        os.makedirs(f'dataset/{gesture}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6b5e9-610b-49fb-b553-9267ea529c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture images for each gesture \n",
    "cap = cv2.VideoCapture(0) \n",
    " \n",
    "for gesture in gestures: \n",
    "    print(f\"Capturing images for {gesture}\") \n",
    "    for img_num in range(100):  # Capture 100 images per gesture \n",
    "        ret, frame = cap.read() \n",
    "        if not ret: \n",
    "            break \n",
    "        cv2.putText(frame, f\"Sign: {gesture}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2) \n",
    "        cv2.imshow(\"Frame\", frame) \n",
    "        cv2.imwrite(f'dataset/{gesture}/{img_num}.jpg', frame) \n",
    "         \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break \n",
    " \n",
    "cap.release() \n",
    "cv2.destroyAllWindows() \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f095a-2d1b-47ae-b9dd-386201e383dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "data = [] \n",
    "labels = [] \n",
    "for gesture in gestures: \n",
    "for img_name in os.listdir(f'dataset/{gesture}'): \n",
    "img = cv2.imread(f'dataset/{gesture}/{img_name}') \n",
    "img = cv2.resize(img, (64, 64))  # Resize to 64x64 \n",
    "data.append(img) \n",
    "labels.append(gestures.index(gesture)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c56d1-bee8-45a0-ac07-1455604eae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays \n",
    "data = np.array(data) / 255.0  # Normalize pixel values to [0, 1] \n",
    "labels = np.array(labels) \n",
    "# Shuffle the data \n",
    "from sklearn.utils import shuffle \n",
    "data, labels = shuffle(data, labels, random_state=42) \n",
    "# Split into train and test sets \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42) \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0726d2-f311-47e3-9462-232a252d607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model \n",
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3))) \n",
    "model.add(MaxPooling2D((2, 2))) \n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2))) \n",
    "model.add(Conv2D(128, (3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2))) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(128, activation='relu')) \n",
    "model.add(Dense(len(gestures), activation='softmax')) \n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n",
    "# Train the model \n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test)) \n",
    "# Evaluate the model \n",
    "test_loss, test_acc = model.evaluate(X_test, y_test) \n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb8472-d22d-4300-980b-afc58bc8042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting a new sign \n",
    "def predict_sign(image): \n",
    "img = cv2.resize(image, (64, 64)) \n",
    "img = np.expand_dims(img, axis=0) \n",
    "img = img / 255.0 \n",
    "prediction = model.predict(img) \n",
    "return gestures[np.argmax(prediction)] \n",
    "cap = cv2.VideoCapture(0) \n",
    "while True: \n",
    "ret, frame = cap.read() \n",
    "if not ret: \n",
    "break \n",
    "# Make a prediction \n",
    "sign = predict_sign(frame) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c6b7c-51d0-49c9-8e0e-59e608ea92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the prediction \n",
    "cv2.putText(frame, f\"Predicted: {sign}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2) \n",
    "cv2.imshow(\"Frame\", frame) \n",
    "if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "break \n",
    "cap.release() \n",
    "cv2.destroyAllWindows() \n",
    "import cv2 \n",
    "import numpy as np \n",
    "from collections import deque \n",
    "# Initialize an empty list to store the predicted letters \n",
    "predicted_word = deque(maxlen=50)  # Using deque to store up to 50 characters \n",
    "# Function to predict sign from an image \n",
    "def predict_sign(image): \n",
    "img = cv2.resize(image, (64, 64))  # Resize to match the input size \n",
    "img = np.expand_dims(img, axis=0)  # Add an extra dimension (batch size) \n",
    "img = img / 255.0  # Normalize \n",
    "prediction = model.predict(img) \n",
    "return gestures[np.argmax(prediction)]  # Return the gesture with the highest probability \n",
    "cap = cv2.VideoCapture(0) \n",
    "while True: \n",
    "ret, frame = cap.read() \n",
    "if not ret: \n",
    "break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523d65a-9e5e-4754-a01b-37ce899901c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction for the current frame \n",
    "sign = predict_sign(frame) \n",
    "# Add the predicted sign to the list of letters forming the word \n",
    "if sign != \"None\":  # Assuming 'None' is returned when no valid gesture is detected \n",
    "predicted_word.append(sign) \n",
    "# Join the letters in the deque to form the current word \n",
    "current_word = ''.join(predicted_word) \n",
    "# Display the current word on the video feed \n",
    "cv2.putText(frame, f\"Predicted: {current_word}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, \n",
    "0, 0), 2) \n",
    "cv2.imshow(\"Frame\", frame) \n",
    "# Press 'q' to quit the webcam \n",
    "if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "break \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
